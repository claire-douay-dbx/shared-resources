{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Refresh Metadata KPIs Cache\n",
        "\n",
        "This notebook computes metadata health KPIs and stores them in a cache table for fast retrieval.\n",
        "\n",
        "**Note:** KPIs are computed only for the catalogs defined in `ALLOWED_CATALOGS` in the configuration cell.\n",
        "\n",
        "**Schedule this as a Databricks Job to run:**\n",
        "- Hourly during business hours\n",
        "- Or daily at a specific time\n",
        "\n",
        "**Cache Table:** `asda_metadata_rampup.metadata_population.metadata_kpis`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "STAGING_CATALOG = \"asda_metadata_rampup\"\n",
        "STAGING_SCHEMA = \"metadata_population\"\n",
        "CACHE_TABLE = \"metadata_kpis\"\n",
        "SUGGESTIONS_TABLE = \"metadata_suggestions\"\n",
        "\n",
        "# Catalogs to include in KPI computation\n",
        "ALLOWED_CATALOGS = [\n",
        "    \"example_cat_1\",\n",
        "    \"example_cat_2\",\n",
        "]\n",
        "\n",
        "# Build SQL IN clause for filtering\n",
        "CATALOGS_SQL_LIST = \", \".join([f\"'{c}'\" for c in ALLOWED_CATALOGS])\n",
        "\n",
        "cache_full_name = f\"{STAGING_CATALOG}.{STAGING_SCHEMA}.{CACHE_TABLE}\"\n",
        "suggestions_full_name = f\"{STAGING_CATALOG}.{STAGING_SCHEMA}.{SUGGESTIONS_TABLE}\"\n",
        "\n",
        "print(f\"üìä Refreshing KPIs cache: {cache_full_name}\")\n",
        "print(f\"üìÇ Filtering to catalogs: {', '.join(ALLOWED_CATALOGS)}\")\n",
        "print(f\"‚è∞ Started at: {spark.sql('SELECT current_timestamp()').collect()[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Compute Metadata Coverage Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get total tables across allowed catalogs only\n",
        "total_tables_df = spark.sql(f\"\"\"\n",
        "    SELECT COUNT(DISTINCT CONCAT(table_catalog, '.', table_schema, '.', table_name)) as total_tables\n",
        "    FROM system.information_schema.tables\n",
        "    WHERE table_schema NOT IN ('information_schema', 'system')\n",
        "    AND table_catalog IN ({CATALOGS_SQL_LIST})\n",
        "\"\"\")\n",
        "\n",
        "total_tables = total_tables_df.collect()[0]['total_tables']\n",
        "print(f\"üìã Total tables in allowed catalogs: {total_tables}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get tables with descriptions\n",
        "tables_with_desc_df = spark.sql(f\"\"\"\n",
        "    SELECT COUNT(DISTINCT CONCAT(table_catalog, '.', table_schema, '.', table_name)) as tables_with_desc\n",
        "    FROM system.information_schema.tables\n",
        "    WHERE table_schema NOT IN ('information_schema', 'system')\n",
        "    AND table_catalog IN ({CATALOGS_SQL_LIST})\n",
        "    AND comment IS NOT NULL AND comment != ''\n",
        "\"\"\")\n",
        "\n",
        "tables_with_desc = tables_with_desc_df.collect()[0]['tables_with_desc']\n",
        "pct_with_desc = (tables_with_desc / total_tables * 100) if total_tables > 0 else 0\n",
        "print(f\"üìù Tables with descriptions: {tables_with_desc} ({pct_with_desc:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get tables with all columns having descriptions\n",
        "all_cols_desc_df = spark.sql(f\"\"\"\n",
        "    WITH table_column_counts AS (\n",
        "        SELECT \n",
        "            table_catalog,\n",
        "            table_schema,\n",
        "            table_name,\n",
        "            COUNT(*) as total_columns,\n",
        "            SUM(CASE WHEN comment IS NOT NULL AND comment != '' THEN 1 ELSE 0 END) as described_columns\n",
        "        FROM system.information_schema.columns\n",
        "        WHERE table_schema NOT IN ('information_schema', 'system')\n",
        "        AND table_catalog IN ({CATALOGS_SQL_LIST})\n",
        "        GROUP BY table_catalog, table_schema, table_name\n",
        "    )\n",
        "    SELECT COUNT(*) as tables_all_cols_desc\n",
        "    FROM table_column_counts\n",
        "    WHERE total_columns = described_columns AND total_columns > 0\n",
        "\"\"\")\n",
        "\n",
        "tables_all_cols_desc = all_cols_desc_df.collect()[0]['tables_all_cols_desc']\n",
        "pct_all_cols = (tables_all_cols_desc / total_tables * 100) if total_tables > 0 else 0\n",
        "print(f\"‚úÖ Tables with all columns described: {tables_all_cols_desc} ({pct_all_cols:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get tables with partial column descriptions\n",
        "partial_cols_desc_df = spark.sql(f\"\"\"\n",
        "    WITH table_column_counts AS (\n",
        "        SELECT \n",
        "            table_catalog,\n",
        "            table_schema,\n",
        "            table_name,\n",
        "            COUNT(*) as total_columns,\n",
        "            SUM(CASE WHEN comment IS NOT NULL AND comment != '' THEN 1 ELSE 0 END) as described_columns\n",
        "        FROM system.information_schema.columns\n",
        "        WHERE table_schema NOT IN ('information_schema', 'system')\n",
        "        AND table_catalog IN ({CATALOGS_SQL_LIST})\n",
        "        GROUP BY table_catalog, table_schema, table_name\n",
        "    )\n",
        "    SELECT COUNT(*) as tables_partial_cols_desc\n",
        "    FROM table_column_counts\n",
        "    WHERE described_columns > 0 AND described_columns < total_columns\n",
        "\"\"\")\n",
        "\n",
        "tables_partial_cols_desc = partial_cols_desc_df.collect()[0]['tables_partial_cols_desc']\n",
        "pct_partial = (tables_partial_cols_desc / total_tables * 100) if total_tables > 0 else 0\n",
        "print(f\"‚ö†Ô∏è  Tables with partial column descriptions: {tables_partial_cols_desc} ({pct_partial:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top 3 suggestors with most rejections (normalize usernames by removing email domain)\n",
        "top_rejected_suggestors_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN suggestor_id LIKE '%@%' THEN SPLIT(suggestor_id, '@')[0]\n",
        "            ELSE suggestor_id\n",
        "        END as normalized_user,\n",
        "        COUNT(*) as rejection_count\n",
        "    FROM {suggestions_full_name}\n",
        "    WHERE status = 'rejected'\n",
        "    GROUP BY \n",
        "        CASE \n",
        "            WHEN suggestor_id LIKE '%@%' THEN SPLIT(suggestor_id, '@')[0]\n",
        "            ELSE suggestor_id\n",
        "        END\n",
        "    ORDER BY rejection_count DESC\n",
        "    LIMIT 3\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚ùå Top 3 Suggestors with Most Rejections:\")\n",
        "for row in top_rejected_suggestors_df.collect():\n",
        "    print(f\"   - {row['normalized_user']}: {row['rejection_count']} rejections\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Compute Top Contributors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top 3 suggestors (normalize usernames by removing email domain)\n",
        "top_suggestors_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN suggestor_id LIKE '%@%' THEN SPLIT(suggestor_id, '@')[0]\n",
        "            ELSE suggestor_id\n",
        "        END as normalized_user,\n",
        "        COUNT(*) as suggestion_count\n",
        "    FROM {suggestions_full_name}\n",
        "    GROUP BY \n",
        "        CASE \n",
        "            WHEN suggestor_id LIKE '%@%' THEN SPLIT(suggestor_id, '@')[0]\n",
        "            ELSE suggestor_id\n",
        "        END\n",
        "    ORDER BY suggestion_count DESC\n",
        "    LIMIT 3\n",
        "\"\"\")\n",
        "\n",
        "print(\"üèÜ Top 3 Contributors:\")\n",
        "for row in top_suggestors_df.collect():\n",
        "    print(f\"   - {row['normalized_user']}: {row['suggestion_count']} suggestions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top 3 reviewers (normalize usernames by removing email domain)\n",
        "top_reviewers_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN reviewer_id LIKE '%@%' THEN SPLIT(reviewer_id, '@')[0]\n",
        "            ELSE reviewer_id\n",
        "        END as normalized_user,\n",
        "        COUNT(*) as review_count\n",
        "    FROM {suggestions_full_name}\n",
        "    WHERE reviewer_id IS NOT NULL\n",
        "    GROUP BY \n",
        "        CASE \n",
        "            WHEN reviewer_id LIKE '%@%' THEN SPLIT(reviewer_id, '@')[0]\n",
        "            ELSE reviewer_id\n",
        "        END\n",
        "    ORDER BY review_count DESC\n",
        "    LIMIT 3\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Top 3 Reviewers:\")\n",
        "for row in top_reviewers_df.collect():\n",
        "    print(f\"   - {row['normalized_user']}: {row['review_count']} reviews\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create/Update Cache Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cache table if it doesn't exist\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS {cache_full_name} (\n",
        "        refresh_timestamp TIMESTAMP,\n",
        "        metric_name STRING,\n",
        "        metric_value BIGINT,\n",
        "        metric_percentage DOUBLE,\n",
        "        user_id STRING,\n",
        "        user_count BIGINT,\n",
        "        user_rank INT\n",
        "    )\n",
        "    TBLPROPERTIES (\n",
        "        'delta.enableChangeDataFeed' = 'true',\n",
        "        'description' = 'Cache table for metadata health KPIs - refreshed by scheduled job'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "print(f\"‚úÖ Cache table ready: {cache_full_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for cache table\n",
        "from pyspark.sql.functions import current_timestamp, lit\n",
        "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, LongType, DoubleType, IntegerType\n",
        "\n",
        "refresh_time = spark.sql(\"SELECT current_timestamp() as ts\").collect()[0]['ts']\n",
        "\n",
        "# Create rows for metadata statistics\n",
        "data = [\n",
        "    (refresh_time, 'total_tables', total_tables, None, None, None, None),\n",
        "    (refresh_time, 'tables_with_desc', tables_with_desc, pct_with_desc, None, None, None),\n",
        "    (refresh_time, 'tables_all_cols_desc', tables_all_cols_desc, pct_all_cols, None, None, None),\n",
        "    (refresh_time, 'tables_partial_cols_desc', tables_partial_cols_desc, pct_partial, None, None, None),\n",
        "]\n",
        "\n",
        "# Add top suggestors\n",
        "for idx, row in enumerate(top_suggestors_df.collect(), 1):\n",
        "    data.append((\n",
        "        refresh_time, 'top_suggestor', None, None, \n",
        "        row['normalized_user'], row['suggestion_count'], idx\n",
        "    ))\n",
        "\n",
        "# Add top reviewers\n",
        "for idx, row in enumerate(top_reviewers_df.collect(), 1):\n",
        "    data.append((\n",
        "        refresh_time, 'top_reviewer', None, None,\n",
        "        row['normalized_user'], row['review_count'], idx\n",
        "    ))\n",
        "\n",
        "# Add top rejected suggestors\n",
        "for idx, row in enumerate(top_rejected_suggestors_df.collect(), 1):\n",
        "    data.append((\n",
        "        refresh_time, 'top_rejected_suggestor', None, None,\n",
        "        row['normalized_user'], row['rejection_count'], idx\n",
        "    ))\n",
        "\n",
        "# Create DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"refresh_timestamp\", TimestampType(), True),\n",
        "    StructField(\"metric_name\", StringType(), True),\n",
        "    StructField(\"metric_value\", LongType(), True),\n",
        "    StructField(\"metric_percentage\", DoubleType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"user_count\", LongType(), True),\n",
        "    StructField(\"user_rank\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "cache_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(f\"üìä Prepared {cache_df.count()} rows for cache\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear old data and insert new data\n",
        "spark.sql(f\"DELETE FROM {cache_full_name}\")\n",
        "\n",
        "cache_df.write.mode(\"append\").saveAsTable(cache_full_name)\n",
        "\n",
        "print(f\"‚úÖ Cache table updated successfully!\")\n",
        "print(f\"‚è∞ Refresh completed at: {refresh_time}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify Cache Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display cached data\n",
        "display(spark.sql(f\"SELECT * FROM {cache_full_name} ORDER BY metric_name, user_rank\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "‚úÖ **Cache table refreshed successfully!**\n",
        "\n",
        "The metadata KPIs are now cached in `asda_metadata_rampup.metadata_population.metadata_kpis`.\n",
        "\n",
        "**Catalogs included:** `example_cat_1`, `example_cat_2`\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Schedule this notebook as a Job:**\n",
        "   - Go to Workflows ‚Üí Create Job\n",
        "   - Select this notebook\n",
        "   - Schedule: Hourly or Daily\n",
        "   - Cluster: Small cluster is sufficient\n",
        "\n",
        "2. **The Streamlit app will now read from this cache** for instant KPI display\n",
        "\n",
        "3. **Monitor the refresh:**\n",
        "   ```sql\n",
        "   SELECT MAX(refresh_timestamp) as last_refresh\n",
        "   FROM asda_metadata_rampup.metadata_population.metadata_kpis\n",
        "   ```\n",
        "\n",
        "4. **To modify the catalogs included:** Edit the `ALLOWED_CATALOGS` list in the Configuration cell\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
